\subsection{Replication}
\label{sec:replication}

Fault tolerance and reliability in distributed systems with client-server architecture are generally achieved by data replication: information is shared on redundant server replicas such that any replica can become the new master if the current master fails. While improving system artifacts like fault-tolerance, reliability and availability, replication can come at the cost of performance: depending on the required operations in the system for replication, system performance can suffer significant bottlenecks. Different models of replication have been proposed to trade consistency for performance, resulting in different levels of consistency as a design choice for the target system. Traditionally, two strategies of replication are distinguished: \textit{active replication} and \textit{passive replication}. A third type of replication, \textit{lazy replication}, was later introduced and is gaining more attention recently. The following paragraphs describe these three types of replication.

\textbf{Active replication.} The first strategy (also called \textit{primary-backup} or \textit{master-slave}), requests to the master replica are processed to all other replicas. Given the same initial state and request sequence, all replicas will produce the same response sequence and reach the same final state. Active replication has become most prominent with the introduction of the State Machine Replication model which was introduced in the 1980s \cite{Lamport:1984} and later refined in \cite{Schneider:1990}. It is based on the concept of distributed consensus with the goal of reliably reaching a stable state of the system in the presence of failures. While providing small recovery delay after failures due to an imposed total order of state updates, computation performance can suffer tremendous bottlenecks since updates must be sequentially propagated through all replicas.

\textbf{Passive replication.} The second strategy (also called \textit{multi-primary} or \textit{multi-master} scheme) relaxes sequential ordering: clients communicate with a master replica and updates are forwarded to backup replicas. Computation performance is improved with this pattern since all computation takes place on the master replica and only the results are propagated. The downside of the approach is that more network bandwidth is required if updates are large. Since the primary replica represents a single point of entry to clients with this approach, there must be some kind of distributed concurrency control in order to reliably restore state when the primary fails. This makes the implementation of this approach more complex and recovery potentially slower. One promising approach of passive replication is Chain Replication \cite{VanRenesse:2004}. Whereas traditional topologies resemble stars with the master replica at the center, this approach forms a chain of replicas with the primary being the tail of the chain. This model aims at providing high availability without sacrificing strong consistency. The main advantage is that reads can address one end of the chain and writes the other. While recovery of failing tail or head servers is simple, recovery of failing middle-servers is complex.

\textbf{Lazy replication.} A third strategy of replication was proposed in 1990: \textit{lazy replication} \cite{Ladin:1990,Ladin:1992} (also called \textit{optimistic replication}) aims at providing highest possible performance and availability by sacrificing consistency significantly. With this approach, replicas periodically exchange information, tolerating out-of-sync periods but guarantee to catch up eventually. While the traditional approaches guarantee from the beginning that all replicas have the exact same state at any point in time, lazy replication allows states to diverge on replicas, but guarantees that the states converge when the system quiesces for some time period. In contrast to the strong consistency models used in the traditional approaches, lazy replication is based on eventual consistency which has gained more attention recently, in particular in online editing platforms, NoSQL cloud databases and big data \footnote{http://www.oracle.com/technetwork/consistency-explained-1659908.pdf, accessed 2017-10-08}. Eventual consistency is the weakest consistency model, providing no guarantee for safety as long as replicas have not converged. Rather, it "push[es] the boundaries of highly available systems" \cite{Bailis:2013}. The introduction of \textit{conflict-free replicated data types} \cite{Shapiro:2011} aimed at a stronger model of eventual consistency: any two replicas that receive the same updates, no matter the order, will be in the same state. CFDTs are categorized in operation-based (only update operation is propagated) and state-based (full state is propagated). A number of CFDTs have been suggested, among them are sets, maps and graphs. It is important to mention that all eventual consistency models impact the application designer since she has to determine what level consistency is sufficient for the specific application.



